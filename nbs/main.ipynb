{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Vision: 2D and 3D Mosquito Trajectories can be as Valuable for Behaviour Analysis via Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import tqdm\n",
    "import joblib\n",
    "import openpyxl\n",
    "\n",
    "from scipy import signal, stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from sklearn.metrics import confusion_matrix as con_mat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "sys.path.append('H:/Documents/PhD/mosquito-swarms/2d-anomaly-detection/src')\n",
    "\n",
    "import reading\n",
    "import preprocessing\n",
    "import features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data from Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males, couples = reading.load_all_files()\n",
    "\n",
    "males = preprocessing.remove_low_length_tracks(males)\n",
    "couples = preprocessing.remove_low_length_tracks(couples)\n",
    "\n",
    "\n",
    "def add_velocity(dataset):\n",
    "    count = 0\n",
    "    for trial_index, trial in enumerate(dataset):\n",
    "        for track_index, track in enumerate(trial):\n",
    "            x_dot, y_dot, z_dot = preprocessing.velocity(track)\n",
    "            dataset[trial_index][track_index][:, 3] = np.append(x_dot, [np.nan,np.nan])\n",
    "            dataset[trial_index][track_index][:, 4] = np.append(y_dot, [np.nan,np.nan])\n",
    "            dataset[trial_index][track_index][:, 5] = np.append(z_dot, [np.nan,np.nan])\n",
    "            count += 1 \n",
    "    return dataset\n",
    "\n",
    "males = add_velocity(males)\n",
    "couples = add_velocity(couples)\n",
    "\n",
    "\n",
    "df = pd.read_excel('H:/Documents/PhD/mosquito-swarms/anomalies-in-swarming/data/search-to-pursuit/data.ods')\n",
    "\n",
    "f = df[df['id'] == 'F']\n",
    "fm = df[df['id'] == 'FM']\n",
    "\n",
    "def format(df):\n",
    "    trials = []\n",
    "    for trial in df['seq'].unique():\n",
    "        tracks = []\n",
    "        for mossie_id in df[df['seq'] == trial]['mqid'].unique():\n",
    "            tracks.append(\n",
    "                df[df['seq'] == trial][df['mqid'] == mossie_id][['p1','p2','p3']].values\n",
    "                )\n",
    "        trials.append(tracks)\n",
    "    return trials\n",
    "\n",
    "f_df = format(f)\n",
    "fm_df = format(fm)\n",
    "\n",
    "count = 0\n",
    "for trial_index, trial in enumerate(f_df):\n",
    "    for track_index, track in enumerate(trial):\n",
    "        f_df[trial_index][track_index] = preprocessing.append_vel(track)\n",
    "        count += 1 \n",
    "\n",
    "count = 0\n",
    "for trial_index, trial in enumerate(fm_df):\n",
    "    for track_index, track in enumerate(trial):\n",
    "        fm_df[trial_index][track_index] = preprocessing.append_vel(track)\n",
    "        count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tracks_males.npy', np.array(males))\n",
    "np.save('tracks_couples.npy', np.array(couples))\n",
    "np.save('tracks_females.npy', np.array(f_df))\n",
    "np.save('tracks_focal_males.npy', np.array(fm_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Numpy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '2d stereo model performance'\n",
    "path = f'H:/Documents/PhD/mosquito-swarms/2d-anomaly-detection/results/{model}/'\n",
    "\n",
    "males = np.load(path + 'tracks_males.npy', allow_pickle=True).tolist()\n",
    "couples = np.load(path + 'tracks_couples.npy', allow_pickle=True).tolist()\n",
    "females = np.load(path + 'tracks_females.npy', allow_pickle=True).tolist()\n",
    "focal_males = np.load(path + 'tracks_focal_males.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''REMOVE TRIAL ID 5'''\n",
    "\n",
    "males.pop(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Camera Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Y-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create 2D projected dataset using pinhole camera - NORMAL VIEW'''\n",
    "\n",
    "def get_swarm_centre(trial, dims_3=True):\n",
    "    if dims_3:\n",
    "        return np.mean(np.vstack(trial)[:,:3],axis=0)\n",
    "    return np.mean(np.vstack(trial)[:,:2],axis=0)\n",
    "\n",
    "\n",
    "def get_new_focal_length(u, uf, f):\n",
    "    v = 1/((1/f) - (1/u))\n",
    "    M = v/u\n",
    "    vf = M*uf\n",
    "    return 1/((1/uf) + (1/vf))\n",
    "\n",
    "def get_closest_camera_position_depth(trial):\n",
    "    depth = []\n",
    "    for t in trial:\n",
    "        depth += t[:,0].tolist()\n",
    "    return np.max(depth)\n",
    "\n",
    "\n",
    "def apply_transformation(tracks, dist, far_dist=None):\n",
    "    transformed_trials = []\n",
    "    original_box = []\n",
    "    transformed_box = []\n",
    "    for dist_index, trial in enumerate(tracks):\n",
    "        box_points, box_centre = get_bounding_cuboid_corners(trial)\n",
    "        original_box.append(box_points)\n",
    "\n",
    "        closest_pos, _, _ = get_swarm_centre(trial) # get centre of swarm - note that cz is depth\n",
    "\n",
    "        _, cy, cz = box_centre[0], box_centre[1], box_centre[2]\n",
    "\n",
    "        fx = 1993.207712\n",
    "        fy = 1986.202825\n",
    "        if far_dist is not None:\n",
    "            Xc, Yc, Zc = cy, -cz, closest_pos+far_dist[dist_index]\n",
    "\n",
    "            fx = get_new_focal_length(2000, far_dist[dist_index], fx)\n",
    "            fy = get_new_focal_length(2000, far_dist[dist_index], fy)\n",
    "\n",
    "\n",
    "        else:\n",
    "            Xc, Yc, Zc = cy, -cz, closest_pos+dist[dist_index]\n",
    "\n",
    "        # Obtained from Butail\n",
    "        distortion_coeffs = np.array([-0.088547, 0.292341, 0, 0], dtype=float) \n",
    "        camera_matrix = np.array([\n",
    "                    [fx,  0,   705.234342],\n",
    "                    [0,   fy,  515.751035],\n",
    "                    [0,   0,   1]\n",
    "        ], dtype=float)\n",
    "\n",
    "        camera_rotation = np.array([0, 0, 0], dtype=float)\n",
    "        camera_position = np.array([-Xc, -Yc, -Zc], dtype=float)\n",
    "\n",
    "        transformed_tracks = []\n",
    "        for track in trial:\n",
    "            # Arrange tracks in (across, height, depth)\n",
    "            track_arranged = track[:,[1,2,0]]*np.array([1,-1,-1])\n",
    "            image_points, _ = cv2.projectPoints(track_arranged[:,:3], camera_rotation, camera_position, camera_matrix, distortion_coeffs)\n",
    "            final_track = image_points[:,0,:].astype(np.float64)\n",
    "            transformed_tracks.append(final_track*np.array([-1,1]))\n",
    "        transformed_trials.append(transformed_tracks)\n",
    "\n",
    "        box_points[:,0], box_points[:,1], box_points[:,2] = box_points[:,1], -box_points[:,2], -box_points[:,0]\n",
    "        t_box, _ = cv2.projectPoints(box_points, camera_rotation, camera_position, camera_matrix, distortion_coeffs)\n",
    "        transformed_box.append(t_box[:,0,:].astype(np.float64)*np.array([-1,1]))\n",
    "\n",
    "    return transformed_trials, original_box, transformed_box\n",
    "\n",
    "\n",
    "def add_2d_velocity(dataset):\n",
    "    count = 0\n",
    "    for trial_index, trial in enumerate(dataset):\n",
    "        for track_index, track in enumerate(trial):\n",
    "            try:\n",
    "                x_dot = np.abs((track[2:, 0] - track[:-2, 0])/(2/25))\n",
    "                y_dot = np.abs((track[2:, 1] - track[:-2, 1])/(2/25))\n",
    "                dataset[trial_index][track_index] = np.insert(dataset[trial_index][track_index], len(dataset[trial_index][track_index][0]), np.append(x_dot, [np.nan,np.nan]), axis=1)\n",
    "                dataset[trial_index][track_index] = np.insert(dataset[trial_index][track_index], len(dataset[trial_index][track_index][0]), np.append(y_dot, [np.nan,np.nan]), axis=1)\n",
    "            except:\n",
    "                print(trial_index, track_index)\n",
    "            count += 1 \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_bounding_cuboid_corners(trajectories):\n",
    "    min_coords = np.array([float('inf'), float('inf'), float('inf')])\n",
    "    max_coords = np.array([float('-inf'), float('-inf'), float('-inf')])\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        min_traj = np.min(trajectory, axis=0)\n",
    "        max_traj = np.max(trajectory, axis=0)\n",
    "\n",
    "        min_coords = np.minimum(min_coords, min_traj[:3])\n",
    "        max_coords = np.maximum(max_coords, max_traj[:3])\n",
    "\n",
    "    corners = [\n",
    "        [min_coords[0], min_coords[1], min_coords[2]],\n",
    "        [min_coords[0], min_coords[1], max_coords[2]],\n",
    "        [min_coords[0], max_coords[1], min_coords[2]],\n",
    "        [min_coords[0], max_coords[1], max_coords[2]],\n",
    "        [max_coords[0], min_coords[1], min_coords[2]],\n",
    "        [max_coords[0], min_coords[1], max_coords[2]],\n",
    "        [max_coords[0], max_coords[1], min_coords[2]],\n",
    "        [max_coords[0], max_coords[1], max_coords[2]]\n",
    "    ]\n",
    "    corners = np.array(corners)\n",
    "    centre = np.mean(corners, axis=0)\n",
    "\n",
    "    return corners, centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### X-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create 2D projected dataset using pinhole camera -> X-Z'''\n",
    "\n",
    "# Previous was X_model, Y_model, Z_model = Y_butail, -Z_butail, X_butail\n",
    "# NOW: X_model, Y_model, Z_model = X_butail, -Z_butail, -Y_butail\n",
    "\n",
    "def get_swarm_centre(trial, dims_3=True):\n",
    "    if dims_3:\n",
    "        return np.mean(np.vstack(trial)[:,:3],axis=0)\n",
    "    return np.mean(np.vstack(trial)[:,:2],axis=0)\n",
    "\n",
    "\n",
    "def get_new_focal_length(u, uf, f):\n",
    "    v = 1/((1/f) - (1/u))\n",
    "    M = v/u\n",
    "    vf = M*uf\n",
    "    return 1/((1/uf) + (1/vf))\n",
    "\n",
    "def get_closest_camera_position_depth(trial):\n",
    "    depth = []\n",
    "    for t in trial:\n",
    "        depth += t[:,0].tolist()\n",
    "    return np.max(depth)\n",
    "\n",
    "\n",
    "def apply_transformation(tracks, dist, far_dist=None):\n",
    "    transformed_trials = []\n",
    "    original_box = []\n",
    "    transformed_box = []\n",
    "    for dist_index, trial in enumerate(tracks):\n",
    "        box_points, box_centre = get_bounding_cuboid_corners(trial)\n",
    "        original_box.append(box_points)\n",
    "\n",
    "        #closest_pos = get_closest_camera_position_depth(trial)\n",
    "        _, closest_pos, _ = get_swarm_centre(trial) # get centre of swarm - note that cz is depth\n",
    "\n",
    "        cx, cy, cz = box_centre[0], box_centre[1], box_centre[2]\n",
    "\n",
    "        fx = 1993.207712\n",
    "        fy = 1986.202825\n",
    "        if far_dist is not None:\n",
    "            Xc, Yc, Zc = cx, -cz, -closest_pos+far_dist[dist_index] #<<<< CHANGE FOR NEW ORIENTATION\n",
    "\n",
    "            #fx = get_new_focal_length(dist[dist_index], far_dist[dist_index], fx)\n",
    "            #fy = get_new_focal_length(dist[dist_index], far_dist[dist_index], fy)\n",
    "\n",
    "            fx = get_new_focal_length(2000, far_dist[dist_index], fx)\n",
    "            fy = get_new_focal_length(2000, far_dist[dist_index], fy)\n",
    "\n",
    "\n",
    "        else:\n",
    "            Xc, Yc, Zc = cx, -cz, -closest_pos+dist[dist_index] #<<<< CHANGE FOR NEW ORIENTATION\n",
    "\n",
    "        # Obtained from Butail\n",
    "        distortion_coeffs = np.array([-0.088547, 0.292341, 0, 0], dtype=float) \n",
    "        camera_matrix = np.array([\n",
    "                    [fx,  0,   705.234342],\n",
    "                    [0,   fy,  515.751035],\n",
    "                    [0,   0,   1]\n",
    "        ], dtype=float)\n",
    "\n",
    "        camera_rotation = np.array([0, 0, 0], dtype=float)\n",
    "        camera_position = np.array([-Xc, -Yc, -Zc], dtype=float)\n",
    "\n",
    "        transformed_tracks = []\n",
    "        for track in trial:\n",
    "            # Arrange tracks in (across, height, depth)\n",
    "            track_arranged = track[:,[0,2,1]]*np.array([1,-1,-1]) #<<<< CHANGE FOR NEW ORIENTATION\n",
    "            image_points, _ = cv2.projectPoints(track_arranged[:,:3], camera_rotation, camera_position, camera_matrix, distortion_coeffs)\n",
    "            final_track = image_points[:,0,:].astype(np.float64)\n",
    "            transformed_tracks.append(final_track*np.array([1,-1]))\n",
    "        transformed_trials.append(transformed_tracks)\n",
    "\n",
    "        box_points[:,0], box_points[:,1], box_points[:,2] = box_points[:,1], -box_points[:,2], -box_points[:,0]\n",
    "        t_box, _ = cv2.projectPoints(box_points, camera_rotation, camera_position, camera_matrix, distortion_coeffs)\n",
    "        transformed_box.append(t_box[:,0,:].astype(np.float64)*np.array([-1,1]))\n",
    "\n",
    "    return transformed_trials, original_box, transformed_box\n",
    "\n",
    "\n",
    "def add_2d_velocity(dataset):\n",
    "    count = 0\n",
    "    for trial_index, trial in enumerate(dataset):\n",
    "        for track_index, track in enumerate(trial):\n",
    "            try:\n",
    "                x_dot = np.abs((track[2:, 0] - track[:-2, 0])/(2/25))\n",
    "                y_dot = np.abs((track[2:, 1] - track[:-2, 1])/(2/25))\n",
    "                dataset[trial_index][track_index] = np.insert(dataset[trial_index][track_index], len(dataset[trial_index][track_index][0]), np.append(x_dot, [np.nan,np.nan]), axis=1)\n",
    "                dataset[trial_index][track_index] = np.insert(dataset[trial_index][track_index], len(dataset[trial_index][track_index][0]), np.append(y_dot, [np.nan,np.nan]), axis=1)\n",
    "            except:\n",
    "                print(trial_index, track_index)\n",
    "            count += 1 \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_bounding_cuboid_corners(trajectories):\n",
    "    min_coords = np.array([float('inf'), float('inf'), float('inf')])\n",
    "    max_coords = np.array([float('-inf'), float('-inf'), float('-inf')])\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        min_traj = np.min(trajectory, axis=0)\n",
    "        max_traj = np.max(trajectory, axis=0)\n",
    "\n",
    "        min_coords = np.minimum(min_coords, min_traj[:3])\n",
    "        max_coords = np.maximum(max_coords, max_traj[:3])\n",
    "\n",
    "    corners = [\n",
    "        [min_coords[0], min_coords[1], min_coords[2]],\n",
    "        [min_coords[0], min_coords[1], max_coords[2]],\n",
    "        [min_coords[0], max_coords[1], min_coords[2]],\n",
    "        [min_coords[0], max_coords[1], max_coords[2]],\n",
    "        [max_coords[0], min_coords[1], min_coords[2]],\n",
    "        [max_coords[0], min_coords[1], max_coords[2]],\n",
    "        [max_coords[0], max_coords[1], min_coords[2]],\n",
    "        [max_coords[0], max_coords[1], max_coords[2]]\n",
    "    ]\n",
    "    corners = np.array(corners)\n",
    "    centre = np.mean(corners, axis=0)\n",
    "\n",
    "    return corners, centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### X-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create 2D projected dataset using pinhole camera - X-Y'''\n",
    "\n",
    "# Previous was X_model, Y_model, Z_model = Y_butail, -Z_butail, X_butail\n",
    "# NOW: X_model, Y_model, Z_model = Y_butail, -X_butail, -Z_butail\n",
    "\n",
    "def get_swarm_centre(trial, dims_3=True):\n",
    "    if dims_3:\n",
    "        return np.mean(np.vstack(trial)[:,:3],axis=0)\n",
    "    return np.mean(np.vstack(trial)[:,:2],axis=0)\n",
    "\n",
    "\n",
    "def get_new_focal_length(u, uf, f):\n",
    "    v = 1/((1/f) - (1/u))\n",
    "    M = v/u\n",
    "    vf = M*uf\n",
    "    return 1/((1/uf) + (1/vf))\n",
    "\n",
    "def get_closest_camera_position_depth(trial):\n",
    "    depth = []\n",
    "    for t in trial:\n",
    "        depth += t[:,0].tolist()\n",
    "    return np.max(depth)\n",
    "\n",
    "\n",
    "def apply_transformation(tracks, dist, far_dist=None):\n",
    "    transformed_trials = []\n",
    "    original_box = []\n",
    "    transformed_box = []\n",
    "    for dist_index, trial in enumerate(tracks):\n",
    "        box_points, box_centre = get_bounding_cuboid_corners(trial)\n",
    "        original_box.append(box_points)\n",
    "\n",
    "        #closest_pos = get_closest_camera_position_depth(trial)\n",
    "        _, _, closest_pos = get_swarm_centre(trial) # get centre of swarm - note that cz is depth\n",
    "\n",
    "        cx, cy, cz = box_centre[0], box_centre[1], box_centre[2]\n",
    "\n",
    "        fx = 1993.207712\n",
    "        fy = 1986.202825\n",
    "        if far_dist is not None:\n",
    "            Xc, Yc, Zc = cy, -cx, -(closest_pos+far_dist[dist_index])\n",
    "\n",
    "            #fx = get_new_focal_length(dist[dist_index], far_dist[dist_index], fx)\n",
    "            #fy = get_new_focal_length(dist[dist_index], far_dist[dist_index], fy)\n",
    "\n",
    "            fx = get_new_focal_length(2000, far_dist[dist_index], fx)\n",
    "            fy = get_new_focal_length(2000, far_dist[dist_index], fy)\n",
    "\n",
    "\n",
    "        else:\n",
    "            Xc, Yc, Zc = cy, -cx, -(closest_pos+dist[dist_index])\n",
    "\n",
    "        # Obtained from Butail\n",
    "        distortion_coeffs = np.array([-0.088547, 0.292341, 0, 0], dtype=float) \n",
    "        camera_matrix = np.array([\n",
    "                    [fx,  0,   705.234342],\n",
    "                    [0,   fy,  515.751035],\n",
    "                    [0,   0,   1]\n",
    "        ], dtype=float)\n",
    "\n",
    "        camera_rotation = np.array([0, 0, 0], dtype=float)\n",
    "        camera_position = np.array([-Xc, -Yc, -Zc], dtype=float)\n",
    "\n",
    "        transformed_tracks = []\n",
    "        for track in trial:\n",
    "            # Arrange tracks in (across, height, depth)\n",
    "            track_arranged = track[:,[1,0,2]]*np.array([1,-1,-1])\n",
    "            image_points, _ = cv2.projectPoints(track_arranged[:,:3], camera_rotation, camera_position, camera_matrix, distortion_coeffs)\n",
    "            final_track = image_points[:,0,:].astype(np.float64)\n",
    "            transformed_tracks.append(final_track*np.array([1,1]))\n",
    "        transformed_trials.append(transformed_tracks)\n",
    "\n",
    "        box_points[:,0], box_points[:,1], box_points[:,2] = box_points[:,1], -box_points[:,2], -box_points[:,0]\n",
    "        t_box, _ = cv2.projectPoints(box_points, camera_rotation, camera_position, camera_matrix, distortion_coeffs)\n",
    "        transformed_box.append(t_box[:,0,:].astype(np.float64)*np.array([-1,1]))\n",
    "\n",
    "    return transformed_trials, original_box, transformed_box\n",
    "\n",
    "\n",
    "def add_2d_velocity(dataset):\n",
    "    count = 0\n",
    "    for trial_index, trial in enumerate(dataset):\n",
    "        for track_index, track in enumerate(trial):\n",
    "            try:\n",
    "                x_dot = np.abs((track[2:, 0] - track[:-2, 0])/(2/25))\n",
    "                y_dot = np.abs((track[2:, 1] - track[:-2, 1])/(2/25))\n",
    "                dataset[trial_index][track_index] = np.insert(dataset[trial_index][track_index], len(dataset[trial_index][track_index][0]), np.append(x_dot, [np.nan,np.nan]), axis=1)\n",
    "                dataset[trial_index][track_index] = np.insert(dataset[trial_index][track_index], len(dataset[trial_index][track_index][0]), np.append(y_dot, [np.nan,np.nan]), axis=1)\n",
    "            except:\n",
    "                print(trial_index, track_index)\n",
    "            count += 1 \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_bounding_cuboid_corners(trajectories):\n",
    "    min_coords = np.array([float('inf'), float('inf'), float('inf')])\n",
    "    max_coords = np.array([float('-inf'), float('-inf'), float('-inf')])\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        min_traj = np.min(trajectory, axis=0)\n",
    "        max_traj = np.max(trajectory, axis=0)\n",
    "\n",
    "        min_coords = np.minimum(min_coords, min_traj[:3])\n",
    "        max_coords = np.maximum(max_coords, max_traj[:3])\n",
    "\n",
    "    corners = [\n",
    "        [min_coords[0], min_coords[1], min_coords[2]],\n",
    "        [min_coords[0], min_coords[1], max_coords[2]],\n",
    "        [min_coords[0], max_coords[1], min_coords[2]],\n",
    "        [min_coords[0], max_coords[1], max_coords[2]],\n",
    "        [max_coords[0], min_coords[1], min_coords[2]],\n",
    "        [max_coords[0], min_coords[1], max_coords[2]],\n",
    "        [max_coords[0], max_coords[1], min_coords[2]],\n",
    "        [max_coords[0], max_coords[1], max_coords[2]]\n",
    "    ]\n",
    "    corners = np.array(corners)\n",
    "    centre = np.mean(corners, axis=0)\n",
    "\n",
    "    return corners, centre"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Camera Model at 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.array([2000,2000,2000,2000,2000,2000,2000,2000,2000,2000,2000]) \n",
    "males, box_males1, tbox_males1 = apply_transformation(males, dist)\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000,2000,2000,3000,2000])\n",
    "couples, box_couples1, tbox_couples1  = apply_transformation(couples, dist)\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000])\n",
    "f_df, box_f1, tbox_f1 = apply_transformation(f_df, dist)\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000])\n",
    "fm_df, box_fm1, tbox_fm1 = apply_transformation(fm_df, dist)\n",
    "\n",
    "males = add_2d_velocity(males)\n",
    "couples = add_2d_velocity(couples)\n",
    "f_df = add_2d_velocity(f_df)\n",
    "fm_df = add_2d_velocity(fm_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Camera Model at other distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_value = 9000\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000,2000,2000,2000,2000,2000])\n",
    "far_dist = np.array([distance_value]*len(dist))\n",
    "males, box_males1, tbox_males1 = apply_transformation(males, dist, far_dist=far_dist)\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000,2000,2000,3000,2000])\n",
    "far_dist = np.array([distance_value]*len(dist))\n",
    "couples, box_couples1, tbox_couples1  = apply_transformation(couples, dist, far_dist=far_dist)\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000])\n",
    "far_dist = np.array([distance_value]*len(dist))\n",
    "f_df, box_f1, tbox_f1 = apply_transformation(f_df, dist, far_dist=far_dist)\n",
    "\n",
    "dist = np.array([2000,2000,2000,2000,2000,2000])\n",
    "far_dist = np.array([distance_value]*len(dist))\n",
    "fm_df, box_fm1, tbox_fm1 = apply_transformation(fm_df, dist, far_dist=far_dist)\n",
    "\n",
    "males = add_2d_velocity(males)\n",
    "couples = add_2d_velocity(couples)\n",
    "f_df = add_2d_velocity(f_df)\n",
    "fm_df = add_2d_velocity(fm_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Telecentric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Y-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONVERT FROM 3D TO 2D TRACKS'''\n",
    "# indexes ->0  1  2   3     4     5\n",
    "# tracks = [x, y, z, xdot, ydot, zdot]\n",
    "\n",
    "def reshape_track(tracks):\n",
    "    track_length = len(tracks[0])\n",
    "    updated_tracks = []\n",
    "    for index in range(track_length):\n",
    "        updated_tracks.append(np.array([tracks[0][index], tracks[1][index], tracks[2][index], tracks[3][index]]))\n",
    "    return np.array(updated_tracks)\n",
    "\n",
    "def convert_from_3d_to_2d(tracks):\n",
    "    converted_tracks = []\n",
    "    for trial in tracks:\n",
    "        conv_trials = []\n",
    "        for track in trial:\n",
    "            conv_trials.append(reshape_track([track[:, 1], track[:, 2], track[:, 4], track[:, 5]]))\n",
    "        converted_tracks.append(conv_trials)\n",
    "    return converted_tracks\n",
    "\n",
    "males = convert_from_3d_to_2d(males)\n",
    "couples = convert_from_3d_to_2d(couples)\n",
    "f_df = convert_from_3d_to_2d(f_df)\n",
    "fm_df = convert_from_3d_to_2d(fm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### X-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONVERT FROM 3D TO 2D TRACKS'''\n",
    "# indexes ->0  1  2   3     4     5\n",
    "# tracks = [x, y, z, xdot, ydot, zdot]\n",
    "\n",
    "def reshape_track(tracks):\n",
    "    track_length = len(tracks[0])\n",
    "    updated_tracks = []\n",
    "    for index in range(track_length):\n",
    "        updated_tracks.append(np.array([tracks[0][index], tracks[1][index], tracks[2][index], tracks[3][index]]))\n",
    "    return np.array(updated_tracks)\n",
    "\n",
    "def convert_from_3d_to_2d(tracks):\n",
    "    converted_tracks = []\n",
    "    for trial in tracks:\n",
    "        conv_trials = []\n",
    "        for track in trial:\n",
    "            conv_trials.append(reshape_track([-track[:, 0], -track[:, 2], track[:, 3], track[:, 5]]))\n",
    "        converted_tracks.append(conv_trials)\n",
    "    return converted_tracks\n",
    "\n",
    "males = convert_from_3d_to_2d(males)\n",
    "couples = convert_from_3d_to_2d(couples)\n",
    "f_df = convert_from_3d_to_2d(f_df)\n",
    "fm_df = convert_from_3d_to_2d(fm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### X-Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONVERT FROM 3D TO 2D TRACKS'''\n",
    "# indexes ->0  1  2   3     4     5\n",
    "# tracks = [x, y, z, xdot, ydot, zdot]\n",
    "\n",
    "def reshape_track(tracks):\n",
    "    track_length = len(tracks[0])\n",
    "    updated_tracks = []\n",
    "    for index in range(track_length):\n",
    "        updated_tracks.append(np.array([tracks[0][index], tracks[1][index], tracks[2][index], tracks[3][index]]))\n",
    "    return np.array(updated_tracks)\n",
    "\n",
    "def convert_from_3d_to_2d(tracks):\n",
    "    converted_tracks = []\n",
    "    for trial in tracks:\n",
    "        conv_trials = []\n",
    "        for track in trial:\n",
    "            conv_trials.append(reshape_track([track[:, 1], -track[:, 0], track[:, 3], -track[:, 4]]))\n",
    "        converted_tracks.append(conv_trials)\n",
    "    return converted_tracks\n",
    "\n",
    "males2 = convert_from_3d_to_2d(males)\n",
    "couples = convert_from_3d_to_2d(couples)\n",
    "f_df = convert_from_3d_to_2d(f_df)\n",
    "fm_df = convert_from_3d_to_2d(fm_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('tracks_males.npy', np.array(males))\n",
    "np.save('tracks_couples.npy', np.array(couples))\n",
    "np.save('tracks_females.npy', np.array(f_df))\n",
    "np.save('tracks_focal_males.npy', np.array(fm_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'H:/Documents/PhD/mosquito-swarms/2d-anomaly-detection/results/2d single far model performance/'\n",
    "\n",
    "males = np.load(path + 'tracks_males.npy', allow_pickle=True)\n",
    "couples = np.load(path + 'tracks_couples.npy', allow_pickle=True)\n",
    "f_df = np.load(path + 'tracks_females.npy', allow_pickle=True)\n",
    "fm_df = np.load(path + 'tracks_focal_males.npy', allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separating tracks using Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Seperating tracks using a sliding window method'''\n",
    "\n",
    "def window(track, size, over_lap):        \n",
    "    start = 0\n",
    "    end = size\n",
    "    tracks = []\n",
    "    while end < len(track):\n",
    "        tracks.append(\n",
    "            track[start:end]\n",
    "        )\n",
    "        start += (size - over_lap)\n",
    "        end += (size - over_lap)\n",
    "    return tracks\n",
    "\n",
    "def segment_tracks(dataset, size, over_lap, min_length):\n",
    "    split_dataset = []\n",
    "    trial_id = 0\n",
    "    while trial_id < len(dataset):\n",
    "        track_id = 0\n",
    "        trial = []\n",
    "        while track_id < len(dataset[trial_id]):\n",
    "            if (len(dataset[trial_id][track_id]) > size) and (len(dataset[trial_id][track_id]) >= min_length):\n",
    "                w = window(dataset[trial_id][track_id], size, over_lap)\n",
    "                if len(w) >= 1:\n",
    "                    trial.append(w)\n",
    "            track_id += 1\n",
    "        trial_id += 1\n",
    "        if trial != []:\n",
    "            split_dataset.append(trial)\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "SIZE = 36\n",
    "OVER_LAP = 7\n",
    "min_length = 75\n",
    "\n",
    "split_males = segment_tracks(males, SIZE, OVER_LAP, min_length=min_length)\n",
    "split_couples = segment_tracks(couples, SIZE, OVER_LAP, min_length=min_length)\n",
    "split_females = segment_tracks(f_df, SIZE, OVER_LAP, min_length=min_length)\n",
    "split_focal_males = segment_tracks(fm_df, SIZE, OVER_LAP, min_length=min_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(dataset):\n",
    "    trial_id = 0\n",
    "    while trial_id < len(dataset):\n",
    "        track_group = 0\n",
    "        while track_group < len(dataset[trial_id]):\n",
    "            track_id = 0\n",
    "            while track_id < len(dataset[trial_id][track_group]):\n",
    "                # Angular Velocity \n",
    "                av_1 = features.angular_velocity(dataset[trial_id][track_group][track_id], (0,1), time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(av_1, [np.nan, np.nan]), axis=1)\n",
    "                \n",
    "                # Angular Acceleration \n",
    "                aa_1 = features.angular_acceleration(dataset[trial_id][track_group][track_id], (0,1), time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(aa_1, [np.nan, np.nan, np.nan]), axis=1)\n",
    "                \n",
    "                # Direction of Flight Change \n",
    "                dof_1 = features.direction_of_flight_change(dataset[trial_id][track_group][track_id], (0,1))\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(dof_1, [np.nan,np.nan]), axis=1)\n",
    "\n",
    "                # Centroid Distance Function\n",
    "                centroid_distance_function = features.centroid_distance_function(dataset[trial_id][track_group][track_id], (0,1))\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), centroid_distance_function, axis=1)\n",
    "\n",
    "                # Orthogonal Components \n",
    "                pv, tv = features.orthogonal_components(dataset[trial_id][track_group][track_id], (0,1), time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(pv, [np.nan, np.nan, np.nan]), axis=1)\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(tv, [np.nan, np.nan, np.nan]), axis=1)\n",
    "                \n",
    "                \n",
    "                # Absolute radial velocity\n",
    "                radial_velocity = np.abs(features.velocity(dataset[trial_id][track_group][track_id], (0,1), time_step=(1/25), schema='central'))\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(radial_velocity, [np.nan, np.nan]), axis=1)\n",
    "\n",
    "                # Absolute radial acceleration\n",
    "                radial_acceleration = features.acceleration(dataset[trial_id][track_group][track_id], (0,1), time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(radial_acceleration, [np.nan, np.nan, np.nan]), axis=1)\n",
    "\n",
    "                # Absolute radial jerk\n",
    "                jerk = features.jerk(dataset[trial_id][track_group][track_id], (0,1), time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(jerk, [np.nan, np.nan, np.nan, np.nan]), axis=1)\n",
    "\n",
    "                # Y axial acceleration\n",
    "                y = features.axial_acceleration(dataset[trial_id][track_group][track_id], 0, time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(y, [np.nan, np.nan, np.nan]), axis=1)\n",
    "\n",
    "                # Z axial acceleration\n",
    "                z = features.axial_acceleration(dataset[trial_id][track_group][track_id], 1, time_step=(1/25), schema='central')\n",
    "                dataset[trial_id][track_group][track_id] = np.insert(dataset[trial_id][track_group][track_id], len(dataset[trial_id][track_group][track_id][0]), np.append(z, [np.nan, np.nan, np.nan]), axis=1)\n",
    "\n",
    "                track_id += 1\n",
    "            track_group += 1\n",
    "        trial_id += 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "males = extract(split_males)\n",
    "couples = extract(split_couples)\n",
    "f_df = extract(split_females)\n",
    "fm_df = extract(split_focal_males)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generate statistics of each metric from each position'''\n",
    "\n",
    "def track_stats(track, indexes, columns):\n",
    "    f_stats = dict()\n",
    "    for index, col in enumerate(columns):\n",
    "        elements = track[:, indexes[index]]\n",
    "        elements = elements[~np.isinf(elements)]\n",
    "        elements = elements[~np.isnan(elements)]\n",
    "        f_stats[col + ' (mean)'] = np.mean(elements)\n",
    "        f_stats[col + ' (median)'] = np.median(elements)\n",
    "        f_stats[col + ' (standard deviation)'] = np.std(elements)\n",
    "        f_stats[col + ' (kurtosis)'] = stats.kurtosis(elements)\n",
    "        f_stats[col + ' (skewness)'] = stats.skew(elements)\n",
    "        f_stats[col + ' (no of local minima)'] = signal.argrelextrema(elements, np.less)[0].shape[0]\n",
    "        f_stats[col + ' (no of local maxima)'] = signal.argrelextrema(elements, np.greater)[0].shape[0]\n",
    "        f_stats[col + ' (no of zero-crossings)'] = len(np.where(np.diff(np.sign(elements)))[0])\n",
    "        try:\n",
    "            f_stats[col + ' (1st quartile)'] = np.percentile(elements, 25)\n",
    "            f_stats[col + ' (3rd quartile)'] = np.percentile(elements, 75)\n",
    "        except:\n",
    "            f_stats[col + ' (1st quartile)'] = np.nan\n",
    "            f_stats[col + ' (3rd quartile)'] = np.nan\n",
    "\n",
    "    return f_stats\n",
    "\n",
    "\n",
    "def remove_nans(df):\n",
    "    columns_to_drop = df.columns.to_series()[np.isinf(df).any()]\n",
    "    for column in columns_to_drop:\n",
    "        df = df.drop(columns=str(column))\n",
    "\n",
    "    columns_to_drop = df.columns.to_series()[np.isnan(df).any()]\n",
    "    for column in columns_to_drop:\n",
    "        df = df.drop(columns=str(column))\n",
    "    \n",
    "    indexes = df[df.isna().any(axis=1)].index\n",
    "    df = df.drop(index=indexes)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_segment_statistics(dictionary, dataset, indexes, feature_columns):\n",
    "    for trial in dataset:\n",
    "        for group in trial:\n",
    "            for track in group:\n",
    "                data = track_stats(track, indexes=indexes, columns=feature_columns)\n",
    "                for d in data:\n",
    "                    dictionary[d].append(data[d])\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def add_other_features(data, pos):\n",
    "    def stats(features):\n",
    "        return np.mean(features), np.std(features) \n",
    "\n",
    "    features_dict = {\n",
    "        'Straightness': [],\n",
    "        'Convex Hull (area)': [],\n",
    "        'Convex Hull (perimeter)': [],\n",
    "        'Curvature Scale Space (mean)': [],\n",
    "        'Curvature Scale Space (standard deviation)': [],\n",
    "        'Fractal Dimension': [],\n",
    "        'Curvature Y-Z (mean)': [],\n",
    "        'Curvature Y-Z (standard deviation)': []\n",
    "    }\n",
    "\n",
    "    trial_id = 0\n",
    "    while trial_id < len(data):\n",
    "        track_group = 0\n",
    "        while track_group < len(data[trial_id]):\n",
    "            track_id = 0\n",
    "            while track_id < len(data[trial_id][track_group]):\n",
    "                features_dict['Straightness'].append(features.straightness(data[trial_id][track_group][track_id], pos))\n",
    "                features_dict['Convex Hull (area)'].append(features.convex_hull_area(data[trial_id][track_group][track_id], pos))\n",
    "                features_dict['Convex Hull (perimeter)'].append(features.convex_hull_perimeter(data[trial_id][track_group][track_id], pos))\n",
    "                features_dict['Fractal Dimension'].append(features.fractal_dimension(data[trial_id][track_group][track_id], pos))\n",
    "                css_mean, css_std = stats(features.curvature_scale_space(data[trial_id][track_group][track_id], pos))\n",
    "                features_dict['Curvature Scale Space (mean)'].append(css_mean)\n",
    "                features_dict['Curvature Scale Space (standard deviation)'].append(css_std)\n",
    "                c2_mean, c2_std = stats(features.curvature(data[trial_id][track_group][track_id], (1,2), time_step=(1/25)))\n",
    "                features_dict['Curvature Y-Z (mean)'].append(c2_mean)\n",
    "                features_dict['Curvature Y-Z (standard deviation)'].append(c2_std)\n",
    "                track_id += 1\n",
    "            track_group += 1\n",
    "        trial_id += 1\n",
    "    \n",
    "    feat_df = pd.DataFrame(data=features_dict)\n",
    "    return feat_df\n",
    "\n",
    "\n",
    "feature_columns = [\n",
    "    'Y Velocity',\n",
    "    'Z Velocity',\n",
    "    'Angular Velocity Y-Z',\n",
    "    'Angular Acceleration Y-Z',\n",
    "    'Angle of Flight', \n",
    "    'Centroid Distance Function',\n",
    "    'Persistence Velocity',\n",
    "    'Turning Velocity',\n",
    "    'Radial Velocity',\n",
    "    'Radial Acceleration',\n",
    "    'Radial Jerk',\n",
    "    'Y Acceleration',\n",
    "    'Z Acceleration'\n",
    "]   \n",
    "\n",
    "indexes = [i for i in range(2, len(feature_columns)+2)]  \n",
    "feature_stats = ['mean','median','standard deviation', 'kurtosis', 'skewness','no of local minima','no of local maxima','no of zero-crossings', '1st quartile', '3rd quartile'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_track_statistics = dict()\n",
    "couple_track_statistics = dict()\n",
    "female_track_statistics = dict()\n",
    "focal_male_track_statistics = dict()\n",
    "\n",
    "for col in feature_columns:\n",
    "    for stat in feature_stats:\n",
    "        male_track_statistics[f'{col} ({stat})'] = []\n",
    "        couple_track_statistics[f'{col} ({stat})'] = []\n",
    "        female_track_statistics[f'{col} ({stat})'] = []\n",
    "        focal_male_track_statistics[f'{col} ({stat})'] = []\n",
    "\n",
    "male_track_statistics = compute_segment_statistics(male_track_statistics, males, indexes, feature_columns)\n",
    "couple_track_statistics = compute_segment_statistics(couple_track_statistics, couples, indexes, feature_columns)\n",
    "female_track_statistics = compute_segment_statistics(female_track_statistics, f_df, indexes, feature_columns)\n",
    "focal_male_track_statistics = compute_segment_statistics(focal_male_track_statistics, fm_df, indexes, feature_columns)\n",
    "\n",
    "\n",
    "df_males = pd.DataFrame(data=male_track_statistics)\n",
    "df_couples = pd.DataFrame(data=couple_track_statistics)\n",
    "df_f = pd.DataFrame(data=female_track_statistics)\n",
    "df_fm = pd.DataFrame(data=focal_male_track_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add other track features'''\n",
    "\n",
    "df_males = pd.concat([df_males, add_other_features(males, (0,1))], axis=1)\n",
    "df_couples = pd.concat([df_couples, add_other_features(couples, (0,1))], axis=1)\n",
    "df_f = pd.concat([df_f, add_other_features(f_df, (0,1))], axis=1)\n",
    "df_fm = pd.concat([df_fm, add_other_features(fm_df, (0,1))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_males = remove_nans(df_males)\n",
    "df_couples = remove_nans(df_couples)\n",
    "df_f = remove_nans(df_f)\n",
    "df_fm = remove_nans(df_fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = list(set(df_couples.columns.values) & set(df_males.columns.values) & set(df_f.columns.values) & set(df_fm.columns.values))\n",
    "df_couples = df_couples[similar]\n",
    "df_males = df_males[similar]\n",
    "df_f = df_f[similar]\n",
    "df_fm = df_fm[similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding trials to Couple data for FS - test split'''\n",
    "\n",
    "def add_track_and_trial_id(df, dataset):\n",
    "    seq = []\n",
    "    track_group = []\n",
    "    count = 0\n",
    "    for trial in range(len(dataset)):\n",
    "        for group in range(len(dataset[trial])):\n",
    "            for i in range(len(dataset[trial][group])):\n",
    "                seq.append(trial)\n",
    "                track_group.append(count)\n",
    "            count += 1\n",
    "    df['seq'] = seq\n",
    "    df['track_group'] = track_group\n",
    "    return df\n",
    "\n",
    "df_males = add_track_and_trial_id(df_males, split_males)\n",
    "df_couples = add_track_and_trial_id(df_couples, split_couples)\n",
    "df_f = add_track_and_trial_id(df_f, split_females)\n",
    "df_fm = add_track_and_trial_id(df_fm, split_focal_males)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_males = df_males.dropna()\n",
    "df_couples = df_couples.dropna()\n",
    "df_f = df_f.dropna()\n",
    "df_fm = df_fm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split Couple data'''\n",
    "df_males_fs = df_males[(df_males['seq'] == 4) | (df_males['seq'] == 9) | (df_males['seq'] == 3)]\n",
    "df_males_test = df_males[~((df_males['seq'] == 4) | (df_males['seq'] == 9) | (df_males['seq'] == 3))]\n",
    "df_couple_fs = df_couples[(df_couples['seq'] == 2) | (df_couples['seq'] == 6) ].drop(columns=['seq'])\n",
    "df_couple_test = df_couples[~((df_couples['seq'] == 2) | (df_couples['seq'] == 6))].drop(columns=['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_males_fs), len(df_couple_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conducting U-Test to determine whether there is any statistical difference between couples and males trajectory'''\n",
    "\n",
    "x = df_males_fs.drop(columns=['seq', 'track_group'], errors='ignore')\n",
    "y = df_couple_fs.drop(columns=['track_group'], errors='ignore')\n",
    "\n",
    "results = stats.mannwhitneyu(x, y)\n",
    "rej, pvals, a1, a2 = multipletests(results[1], alpha=0.01, method='holm')\n",
    "\n",
    "cols = {\"columns\": [], \"p_values\": [], 'raw_p':[]}\n",
    "\n",
    "for index, column in enumerate(x.columns.values):\n",
    "    if rej[index] == True:\n",
    "        cols['columns'].append(column)\n",
    "        cols['p_values'].append(pvals[index])\n",
    "        cols['raw_p'].append(results[1][index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Removing Correlated features'''\n",
    "df_hyp = pd.concat([df_males_fs, df_couple_fs])\n",
    "\n",
    "df_hyp_sorted = df_hyp.reindex(sorted(df_hyp.columns), axis=1)\n",
    "corr_matrix = df_hyp_sorted.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "columns = df_couple_test[cols[\"columns\"]].drop(columns=to_drop, errors='ignore').columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding trial id to male df so that splitting by trial can occur'''\n",
    "\n",
    "seq = []\n",
    "for trial in range(len(split_males)):\n",
    "    for i in range(len(split_males[trial])):\n",
    "        for j in range(len(split_males[trial][i])):\n",
    "            seq.append(trial)\n",
    "\n",
    "df_males['seq'] = seq\n",
    "df_males_test = df_males.drop(index=df_males_fs.index.values)\n",
    "df_males_fs = df_males.drop(index=df_males_test.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if abs(x) < 700:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return 1 if np.sign(x) == 1 else 0\n",
    "\n",
    "def compute_logistic_array(arr):\n",
    "    return [sigmoid(x) for x in arr]\n",
    "\n",
    "def get_mode(_set, predicts, decisions):\n",
    "    predictions = []\n",
    "    avg_decision = []\n",
    "    unique_track_groups = _set.unique()\n",
    "    for val in unique_track_groups:\n",
    "        indexes = np.where(_set == val)\n",
    "        preds = predicts[indexes]\n",
    "        avg_decision.append(np.mean(decisions[indexes]))\n",
    "        preds = np.array(np.sign(np.mean(decisions[indexes])))\n",
    "        preds[preds == 0] = 1\n",
    "        predictions.append(preds)\n",
    "    return predictions, avg_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CROSS VALIDATION \n",
    "'''\n",
    "all_trials = df_males_test['seq'].unique()\n",
    "\n",
    "male_train_predictions = {'predictions': [], 'accuracy': []}\n",
    "male_test_predictions = {'predictions': [], 'accuracy': []}\n",
    "couple_predictions = {'predictions': [],  'accuracy': []}\n",
    "female_predictions = {'predictions': [], 'accuracy': []}\n",
    "focal_male_predictions = {'predictions': [], 'accuracy': []}\n",
    "all_test_predictions = {\n",
    "    'true': [], 'preds': [],\n",
    "    'accuracy': [], 'roc auc':[], \n",
    "    'f1 score (male)':[], 'recall (male)':[], 'precision (male)':[], \n",
    "    'f1 score (non-male)':[], 'recall (non-male)':[], 'precision (non-male)':[],\n",
    "    'f1 score (avg)':[], 'recall (avg)':[], 'precision (avg)':[], }\n",
    "roc_curve = {'fpr': [], 'tpr': []}\n",
    "pr_curve = {'m_precision': [], 'm_recall': [], 'm_auc':[], 'm_pn':[], 'n_precision': [], 'n_recall': [], 'n_auc':[], 'n_pn':[], 'auc':[]}\n",
    "\n",
    "shap_train = []\n",
    "shap_test = []\n",
    "all_targets = []\n",
    "\n",
    "seq_test = []\n",
    "seq_train = []\n",
    "\n",
    "nu = 0.2\n",
    "kernel = 'rbf'\n",
    "gamma = (1/len(columns))**2\n",
    "degree = 3\n",
    "\n",
    "a1 = all_trials\n",
    "a2 = all_trials\n",
    "for val1 in tqdm.tqdm(a1):\n",
    "    a2 = a2[a2 != val1]\n",
    "    for val2 in a2:\n",
    "        if val1 != val2:\n",
    "            train = df_males_test[~((df_males_test['seq'] == val1) | (df_males_test['seq'] == val2))]\n",
    "            test = df_males_test[((df_males_test['seq'] == val1) | (df_males_test['seq'] == val2))]\n",
    "\n",
    "            seq_test.append([val1, val2])\n",
    "            seq_train.append([x for x in all_trials if x not in [val1, val2]])\n",
    "    \n",
    "            scaler = StandardScaler()\n",
    "            train_standard = scaler.fit_transform(train[columns])\n",
    "            train_standard = pd.DataFrame(train_standard, index=train.index, columns=columns)\n",
    "\n",
    "            test_standard = scaler.transform(test[columns])\n",
    "            test_standard = pd.DataFrame(test_standard, index=test.index, columns=columns)\n",
    "\n",
    "            df_couple_test_standard = scaler.transform(df_couple_test[columns])\n",
    "            df_couple_test_standard = pd.DataFrame(df_couple_test_standard, index=df_couple_test.index, columns=columns)\n",
    "\n",
    "            df_f_scaler = scaler.transform(df_f[columns])\n",
    "            df_f_scaler = pd.DataFrame(df_f_scaler, index=df_f.index, columns=columns)\n",
    "\n",
    "            focal_male_standard = scaler.transform(df_fm[columns])\n",
    "            focal_male_standard = pd.DataFrame(focal_male_standard, index=df_fm.index, columns=columns)\n",
    "\n",
    "            clf = OneClassSVM(\n",
    "                nu=nu,\n",
    "                kernel=kernel,\n",
    "                gamma=gamma,\n",
    "                degree=degree\n",
    "            ).fit(train_standard)\n",
    "\n",
    "            # ---------- TRAIN SET ---------- \n",
    "            male_train_predictions['predictions'].append(clf.predict(train_standard))\n",
    "            preds, scores = get_mode(train['track_group'], clf.predict(train_standard), clf.decision_function(train_standard))\n",
    "            male_train_predictions['accuracy'].append(metrics.accuracy_score([1 for l in range(len(preds))], preds))\n",
    "\n",
    "            # ---------- TEST SET ---------- \n",
    "            male_test_predictions['predictions'].append(clf.predict(test_standard))\n",
    "            preds, scores = get_mode(test['track_group'], clf.predict(test_standard), clf.decision_function(test_standard))\n",
    "            male_test_predictions['accuracy'].append(metrics.accuracy_score([1 for l in range(len(preds))], preds))\n",
    "\n",
    "            # ---------- COUPLE SET ---------- \n",
    "            couple_predictions['predictions'].append(clf.predict(df_couple_test_standard))\n",
    "            preds, scores = get_mode(df_couple_test['track_group'], clf.predict(df_couple_test_standard), clf.decision_function(df_couple_test_standard))\n",
    "            couple_predictions['accuracy'].append(metrics.accuracy_score([-1 for l in range(len(preds))], preds))\n",
    "\n",
    "            # ---------- FEMALE SET ---------- \n",
    "            female_predictions['predictions'].append(clf.predict(df_f_scaler))\n",
    "            preds, scores = get_mode(df_f['track_group'], clf.predict(df_f_scaler), clf.decision_function(df_f_scaler))\n",
    "            female_predictions['accuracy'].append(metrics.accuracy_score([-1 for l in range(len(preds))], preds))\n",
    "\n",
    "            # ---------- FOCAL MALE SET ---------- \n",
    "            focal_male_predictions['predictions'].append(clf.predict(focal_male_standard))\n",
    "            preds, scores = get_mode(df_fm['track_group'], clf.predict(focal_male_standard), clf.decision_function(focal_male_standard))\n",
    "            focal_male_predictions['accuracy'].append(metrics.accuracy_score([1 for l in range(len(preds))], preds))\n",
    "\n",
    "            # ------------ ALL TEST SET ------------\n",
    "            full_test_set = pd.concat([test[columns], df_couple_test[columns], df_f[columns], df_fm[columns]])\n",
    "            full_test_set_scaled = scaler.transform(full_test_set)\n",
    "            full_test_set = pd.DataFrame(full_test_set_scaled, index=full_test_set.index, columns=full_test_set.columns)\n",
    "            full_test_groups = pd.concat([\n",
    "                test['track_group'].apply(lambda row: f'm{row}'),\n",
    "                df_couple_test['track_group'].apply(lambda row: f'c{row}'),\n",
    "                df_f['track_group'].apply(lambda row: f'f{row}'),\n",
    "                df_fm['track_group'].apply(lambda row: f'fm{row}')])\n",
    "\n",
    "            all_track_targets = [1 for _ in range(len(test['track_group'].unique()))] + [-1 for _ in range(len(df_couple_test['track_group'].unique()))] + [-1 for _ in range(len(df_f['track_group'].unique()))] + [1 for _ in range(len(df_fm['track_group'].unique()))]\n",
    "            all_segment_targets = [1 for _ in range(len(test))] + [-1 for _ in range(len(df_couple_test))] + [-1 for _ in range(len(df_f))] + [1 for _ in range(len(df_fm))]\n",
    "\n",
    "            preds, scores = get_mode(\n",
    "                full_test_groups,\n",
    "                clf.predict(full_test_set), \n",
    "                clf.decision_function(full_test_set), \n",
    "            )\n",
    "            all_test_predictions['true'].append(all_track_targets)\n",
    "            all_test_predictions['preds'].append(preds)\n",
    "\n",
    "            all_test_predictions['accuracy'].append(metrics.balanced_accuracy_score(all_track_targets, preds))\n",
    "            all_test_predictions['roc auc'].append(metrics.roc_auc_score(np.array(all_track_targets), compute_logistic_array(np.array(scores))))\n",
    "\n",
    "            all_test_predictions['f1 score (male)'].append(metrics.f1_score(all_track_targets, preds))\n",
    "            all_test_predictions['recall (male)'].append(metrics.recall_score(all_track_targets, preds))\n",
    "            all_test_predictions['precision (male)'].append(metrics.precision_score(all_track_targets, preds))\n",
    "\n",
    "            all_test_predictions['f1 score (non-male)'].append(metrics.f1_score(all_track_targets, preds, pos_label=-1))\n",
    "            all_test_predictions['recall (non-male)'].append(metrics.recall_score(all_track_targets, preds, pos_label=-1))\n",
    "            all_test_predictions['precision (non-male)'].append(metrics.precision_score(all_track_targets, preds, pos_label=-1))\n",
    "            \n",
    "            all_test_predictions['f1 score (avg)'].append((all_test_predictions['f1 score (male)'][-1] + all_test_predictions['f1 score (non-male)'][-1])/2)\n",
    "            all_test_predictions['recall (avg)'].append((all_test_predictions['recall (male)'][-1] + all_test_predictions['recall (non-male)'][-1])/2)\n",
    "            all_test_predictions['precision (avg)'].append((all_test_predictions['precision (male)'][-1] + all_test_predictions['precision (non-male)'][-1])/2)\n",
    "\n",
    "            fpr, tpr, _ = metrics.roc_curve(np.array(all_track_targets), compute_logistic_array(scores))\n",
    "            roc_curve['fpr'].append(fpr)\n",
    "            roc_curve['tpr'].append(tpr)\n",
    "\n",
    "            shap_train.append(train_standard)\n",
    "            shap_test.append(full_test_set)\n",
    "            all_targets.append(all_segment_targets)\n",
    "\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(all_track_targets, compute_logistic_array(scores))\n",
    "            pr_curve['m_precision'].append(precision)\n",
    "            pr_curve['m_recall'].append(recall)\n",
    "            pr_curve['m_auc'].append(metrics.auc(recall, precision))\n",
    "            pr_curve['m_pn'].append(len([i for i in all_track_targets if i == 1])/len(all_track_targets))\n",
    "\n",
    "            precision, recall, thresholds = metrics.precision_recall_curve(np.array(all_track_targets)*-1, [1 - ar for ar in compute_logistic_array(np.array(scores))])\n",
    "            pr_curve['n_precision'].append(precision)\n",
    "            pr_curve['n_recall'].append(recall)\n",
    "            pr_curve['n_auc'].append(metrics.auc(recall, precision))\n",
    "            pr_curve['n_pn'].append(len([i for i in np.array(all_track_targets) if i == -1])/len(all_track_targets))\n",
    "\n",
    "            pr_curve['auc'].append((pr_curve['m_auc'][-1] + pr_curve['n_auc'][-1])/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/2d-anomaly/FILTER/9m/'\n",
    "\n",
    "np.save(path + 'shap_train.npy', np.array(shap_train))\n",
    "np.save(path + 'shap_test.npy', np.array(shap_test))\n",
    "\n",
    "joblib.dump(all_targets, path + 'targets.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    male_train_predictions,\n",
    "    male_test_predictions,\n",
    "    couple_predictions,\n",
    "    female_predictions,\n",
    "    focal_male_predictions,\n",
    "    all_test_predictions,\n",
    "    roc_curve,\n",
    "    columns\n",
    ")\n",
    "joblib.dump(results, path+'results.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_males_test, df_males_fs, df_couple_test, df_couple_fs, df_f, df_fm]).to_pickle(path+'df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' -- TRAIN SET (MALES) --')\n",
    "print(f'Accuracy: {round(sum(male_train_predictions[\"accuracy\"])/len(male_train_predictions[\"accuracy\"]), 3)} ({round(np.percentile(male_train_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(male_train_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print('\\n -- TEST SET (MALES) --')\n",
    "print(f'Accuracy: {round(sum(male_test_predictions[\"accuracy\"])/len(male_test_predictions[\"accuracy\"]), 3)} ({round(np.percentile(male_test_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(male_test_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print('\\n -- COUPLES --')\n",
    "print(f'Accuracy: {round(sum(couple_predictions[\"accuracy\"])/len(couple_predictions[\"accuracy\"]), 3)} ({round(np.percentile(couple_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(couple_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print('\\n -- FEMALES -- ')\n",
    "print(f'Accuracy: {round(sum(female_predictions[\"accuracy\"])/len(female_predictions[\"accuracy\"]), 3)} ({round(np.percentile(female_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(female_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print('\\n -- FOCAL MALES -- ')\n",
    "print(f'Accuracy: {round(sum(focal_male_predictions[\"accuracy\"])/len(focal_male_predictions[\"accuracy\"]), 3)} ({round(np.percentile(focal_male_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(focal_male_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "\n",
    "print('\\n -- TOTAL DATASET --')\n",
    "print(f'BALANCED ACCURACY: {round(sum(all_test_predictions[\"accuracy\"])/len(all_test_predictions[\"accuracy\"]), 3)} ({round(np.percentile(all_test_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print(f'ROC AUC: {round(sum(all_test_predictions[\"roc auc\"])/len(all_test_predictions[\"roc auc\"]), 3)} ({round(np.percentile(all_test_predictions[\"roc auc\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"roc auc\"], 97.5), 3)})')\n",
    "\n",
    "print(f'\\nF1 SCORE (avg): {round(sum(all_test_predictions[\"f1 score (avg)\"])/len(all_test_predictions[\"f1 score (avg)\"]), 3)} ({round(np.percentile(all_test_predictions[\"f1 score (avg)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"f1 score (avg)\"], 97.5), 3)})')\n",
    "print(f'F1 SCORE (male): {round(sum(all_test_predictions[\"f1 score (male)\"])/len(all_test_predictions[\"f1 score (male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"f1 score (male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"f1 score (male)\"], 97.5), 3)})')\n",
    "print(f'F1 SCORE (non-male): {round(sum(all_test_predictions[\"f1 score (non-male)\"])/len(all_test_predictions[\"f1 score (non-male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"f1 score (non-male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"f1 score (non-male)\"], 97.5), 3)})')\n",
    "\n",
    "print(f'\\nRECALL (avg): {round(sum(all_test_predictions[\"recall (avg)\"])/len(all_test_predictions[\"recall (avg)\"]), 3)} ({round(np.percentile(all_test_predictions[\"recall (avg)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"recall (avg)\"], 97.5), 3)})')\n",
    "print(f'RECALL (male): {round(sum(all_test_predictions[\"recall (male)\"])/len(all_test_predictions[\"recall (male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"recall (male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"recall (male)\"], 97.5), 3)})')\n",
    "print(f'RECALL (non-male): {round(sum(all_test_predictions[\"recall (non-male)\"])/len(all_test_predictions[\"recall (non-male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"recall (non-male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"recall (non-male)\"], 97.5), 3)})')\n",
    "\n",
    "print(f'\\nPRECISION (avg): {round(sum(all_test_predictions[\"precision (avg)\"])/len(all_test_predictions[\"precision (avg)\"]), 3)} ({round(np.percentile(all_test_predictions[\"precision (avg)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"precision (avg)\"], 97.5), 3)})')\n",
    "print(f'PRECISION (male): {round(sum(all_test_predictions[\"precision (male)\"])/len(all_test_predictions[\"precision (male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"precision (male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"precision (male)\"], 97.5), 3)})')\n",
    "print(f'PRECISION (non-male): {round(sum(all_test_predictions[\"precision (non-male)\"])/len(all_test_predictions[\"precision (non-male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"precision (non-male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"precision (non-male)\"], 97.5), 3)})')\n",
    "\n",
    "print(f'\\nPR AUC (avg): {round(sum(pr_curve[\"auc\"])/len(pr_curve[\"auc\"]), 3)} ({round(np.percentile(pr_curve[\"auc\"], 2.5), 3)} - {round(np.percentile(pr_curve[\"auc\"], 97.5), 3)})')\n",
    "print(f'PR AUC (male): {round(sum(pr_curve[\"m_auc\"])/len(pr_curve[\"m_auc\"]), 3)} ({round(np.percentile(pr_curve[\"m_auc\"], 2.5), 3)} - {round(np.percentile(pr_curve[\"m_auc\"], 97.5), 3)})')\n",
    "print(f'PR AUC (non-male): {round(sum(pr_curve[\"n_auc\"])/len(pr_curve[\"n_auc\"]), 3)} ({round(np.percentile(pr_curve[\"n_auc\"], 2.5), 3)} - {round(np.percentile(pr_curve[\"n_auc\"], 97.5), 3)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round(sum(male_train_predictions[\"accuracy\"])/len(male_train_predictions[\"accuracy\"]), 3)} ({round(np.percentile(male_train_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(male_train_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(male_test_predictions[\"accuracy\"])/len(male_test_predictions[\"accuracy\"]), 3)} ({round(np.percentile(male_test_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(male_test_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print(f'{round(sum(couple_predictions[\"accuracy\"])/len(couple_predictions[\"accuracy\"]), 3)} ({round(np.percentile(couple_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(couple_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(female_predictions[\"accuracy\"])/len(female_predictions[\"accuracy\"]), 3)} ({round(np.percentile(female_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(female_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(focal_male_predictions[\"accuracy\"])/len(focal_male_predictions[\"accuracy\"]), 3)} ({round(np.percentile(focal_male_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(focal_male_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(all_test_predictions[\"accuracy\"])/len(all_test_predictions[\"accuracy\"]), 3)} ({round(np.percentile(all_test_predictions[\"accuracy\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"accuracy\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"roc auc\"])/len(all_test_predictions[\"roc auc\"]), 3)} ({round(np.percentile(all_test_predictions[\"roc auc\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"roc auc\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(all_test_predictions[\"f1 score (avg)\"])/len(all_test_predictions[\"f1 score (avg)\"]), 3)} ({round(np.percentile(all_test_predictions[\"f1 score (avg)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"f1 score (avg)\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"f1 score (male)\"])/len(all_test_predictions[\"f1 score (male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"f1 score (male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"f1 score (male)\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"f1 score (non-male)\"])/len(all_test_predictions[\"f1 score (non-male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"f1 score (non-male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"f1 score (non-male)\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(all_test_predictions[\"recall (avg)\"])/len(all_test_predictions[\"recall (avg)\"]), 3)} ({round(np.percentile(all_test_predictions[\"recall (avg)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"recall (avg)\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"recall (male)\"])/len(all_test_predictions[\"recall (male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"recall (male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"recall (male)\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"recall (non-male)\"])/len(all_test_predictions[\"recall (non-male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"recall (non-male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"recall (non-male)\"], 97.5), 3)})')\n",
    "\n",
    "print(f'{round(sum(all_test_predictions[\"precision (avg)\"])/len(all_test_predictions[\"precision (avg)\"]), 3)} ({round(np.percentile(all_test_predictions[\"precision (avg)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"precision (avg)\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"precision (male)\"])/len(all_test_predictions[\"precision (male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"precision (male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"precision (male)\"], 97.5), 3)})')\n",
    "print(f'{round(sum(all_test_predictions[\"precision (non-male)\"])/len(all_test_predictions[\"precision (non-male)\"]), 3)} ({round(np.percentile(all_test_predictions[\"precision (non-male)\"], 2.5), 3)} - {round(np.percentile(all_test_predictions[\"precision (non-male)\"], 97.5), 3)})')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EXCEL FILE OF ALL FOLD SCORES'''\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.create_sheet()\n",
    "\n",
    "cols = ['fold', 'test trials', 'train trials', 'accuracy (male train)', 'accuracy (male test)',\n",
    "    'accuracy (couple test)', 'balanced accuracy (all)', 'roc auc (all)']\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    sheet.cell(row=1, column=i+1).value = col\n",
    "\n",
    "\n",
    "for fold in range(len(all_test_predictions['accuracy'])):\n",
    "    sheet.cell(row=fold+2, column=1).value = fold\n",
    "    sheet.cell(row=fold+2, column=2).value = str(seq_test[fold])\n",
    "    sheet.cell(row=fold+2, column=3).value = str(seq_train[fold])\n",
    "    sheet.cell(row=fold+2, column=4).value = male_train_predictions['accuracy'][fold]\n",
    "    sheet.cell(row=fold+2, column=5).value = male_test_predictions['accuracy'][fold]\n",
    "    sheet.cell(row=fold+2, column=6).value = couple_predictions['accuracy'][fold]\n",
    "    sheet.cell(row=fold+2, column=7).value = all_test_predictions['accuracy'][fold]\n",
    "    sheet.cell(row=fold+2, column=8).value = all_test_predictions['roc auc'][fold]\n",
    "\n",
    "wb.save(path+\"all-folds.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Confusion Matrix'''\n",
    "\n",
    "y_test =  list(chain.from_iterable(all_test_predictions['true']))\n",
    "y_pred =  list(chain.from_iterable(all_test_predictions['preds']))\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.figure(figsize=(8, 6), dpi=400)\n",
    "con = con_mat(y_test, y_pred)\n",
    "cmap = sns.light_palette(\"#dd7301\", as_cmap=True)\n",
    "sns.heatmap(con, annot=True, cmap=cmap, fmt=\".1f\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.xticks([0.5,1.5], ['Non-Male', 'Male'])\n",
    "plt.yticks([0.5,1.5], ['Non-Male', 'Male'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ROC Curve'''\n",
    "\n",
    "plt.figure(figsize=(10,10), dpi=400)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "lw = 2\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "for index in range(len(roc_curve['tpr'])):\n",
    "    plt.plot(\n",
    "        roc_curve['fpr'][index],\n",
    "        roc_curve['tpr'][index],\n",
    "        color=\"blue\",\n",
    "        alpha=0.15,\n",
    "        lw=lw,\n",
    "    )\n",
    "    tpr = np.interp(base_fpr, roc_curve['fpr'][index], roc_curve['tpr'][index])\n",
    "    tpr[0] = 0.0\n",
    "    tprs.append(tpr)\n",
    "\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "\n",
    "plt.plot(base_fpr, mean_tprs, 'b')\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"black\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10), dpi=400)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "lw = 2\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "line_type = ['-', '--', '-.']\n",
    "colours = ['blue', 'green']\n",
    "\n",
    "best = 7\n",
    "worst = 22\n",
    "\n",
    "for i, index in enumerate([best, worst]):\n",
    "    plt.plot(\n",
    "        pr_curve['m_recall'][index],\n",
    "        pr_curve['m_precision'][index],\n",
    "        color=colours[i],\n",
    "        lw=lw,\n",
    "    )\n",
    "    plt.plot(\n",
    "        base_fpr,\n",
    "        [pr_curve['m_pn'][index] for i in base_fpr],\n",
    "        color=colours[i],\n",
    "        alpha=0.4,\n",
    "        linestyle='--'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.legend([f'Best fold (AUC = {round(pr_curve[\"m_auc\"][best], 3)})', f'Best fold baseline (AUC = {round(pr_curve[\"m_pn\"][best], 3)})', f'Worst fold (AUC = {round(pr_curve[\"m_auc\"][worst], 3)}', f'Worst fold baseline (AUC = {round(pr_curve[\"m_pn\"][worst], 3)})'])\n",
    "plt.title(f\"Precision-Recall (PR) Curve (male)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10), dpi=400)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "lw = 2\n",
    "tprs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "line_type = ['-', '--', '-.']\n",
    "colours = ['blue', 'green']\n",
    "\n",
    "best = 7\n",
    "worst = 22\n",
    "\n",
    "for i, index in enumerate([best, worst]):\n",
    "    plt.plot(\n",
    "        pr_curve['n_recall'][index],\n",
    "        pr_curve['n_precision'][index],\n",
    "        color=colours[i],\n",
    "        lw=lw,\n",
    "    )\n",
    "    plt.plot(\n",
    "        base_fpr,\n",
    "        [pr_curve['n_pn'][index] for i in base_fpr],\n",
    "        color=colours[i],\n",
    "        alpha=0.4,\n",
    "        linestyle='--'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.legend([f'Best fold (AUC = {round(pr_curve[\"n_auc\"][best], 3)})', f'Best fold baseline (AUC = {round(pr_curve[\"n_pn\"][best], 3)})', f'Worst fold (AUC = {round(pr_curve[\"n_auc\"][worst], 3)}', f'Worst fold baseline (AUC = {round(pr_curve[\"n_pn\"][worst], 3)})'])\n",
    "plt.title(\"Precision-Recall (PR) Curve (non-male)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
